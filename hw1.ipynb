{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khantimmy27/datasci_223/blob/main/hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43e0c2ad-c4c1-42da-8181-6d76bec34e0e",
      "metadata": {
        "id": "43e0c2ad-c4c1-42da-8181-6d76bec34e0e"
      },
      "source": [
        "Welcome to Homework 1! Your task for this homework is to implement logistic regression from scratch. You are welcome to use existing software packages to check your work. However, implementing it once in your life will help you better understand how this algorithm works.\n",
        "\n",
        "Recall: Logistic regression is a classification model that estimates the probability of a binary outcome $Y$ being equal to 1, given variables/features $X$. It assumes that the log odds is linear with respect to $X$. Because it can be viewed as a generalization of linear regression, it falls under the general umbrella of methods called \"generalizaed linear models.\"\n",
        "\n",
        "Helpful resources:\n",
        "* https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
        "\n",
        "Concepts you'll need from lectures:\n",
        "* Maximum likelihood estimation\n",
        "* Gradient descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e6451ab",
      "metadata": {
        "id": "8e6451ab"
      },
      "source": [
        "### Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "264cfd00-ec55-45ce-b999-3c86a475d348",
      "metadata": {
        "id": "264cfd00-ec55-45ce-b999-3c86a475d348"
      },
      "source": [
        "**How does Logistic Regression work?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecad7236",
      "metadata": {
        "id": "ecad7236"
      },
      "source": [
        "Q1: What is the mathematical equation that describes the probability\n",
        "distribution of a binary random variable? (4 points) [link text](https://)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "P(Y = y) = p^y (1 - p)^{1 - y}, \\quad y \\in \\{0, 1\\}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "vsPa_tOe-R1x"
      },
      "id": "vsPa_tOe-R1x"
    },
    {
      "cell_type": "markdown",
      "id": "f5562197",
      "metadata": {
        "id": "f5562197"
      },
      "source": [
        "Q2: What probability distribution does logistic regression assume $Y|X$ follows? (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression assumes that Y (outcome) given X (features) follows the Bernoulli distribution. The probability of success is calculated using the Sigmoid function."
      ],
      "metadata": {
        "id": "wq0KWZ_G9KMs"
      },
      "id": "wq0KWZ_G9KMs"
    },
    {
      "cell_type": "markdown",
      "id": "4a64e62b",
      "metadata": {
        "id": "4a64e62b"
      },
      "source": [
        "Q3: What are the parameters of a logistic regression model? (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The intercept $(b_0)$ and the coefficients $(b_1, b_2, \\ldots, b_k)$ are the parameters of a logistic regression model.\n"
      ],
      "metadata": {
        "id": "lhFjU3jcDLXD"
      },
      "id": "lhFjU3jcDLXD"
    },
    {
      "cell_type": "markdown",
      "id": "0761a7c9",
      "metadata": {
        "id": "0761a7c9"
      },
      "source": [
        "Q4: What is the log likelihood of the parameters given observations $(X_i, Y_i)$ for $i=1,\\cdots, n$? (8 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[ Y_i \\log(p_i) + (1 - Y_i) \\log(1 - p_i) \\right]\n",
        "$$\n",
        "\n",
        "<p>\n",
        "Where:<br>\n",
        "&emsp;• \\( p_i = \\frac{1}{1 + e^{-\\boldsymbol{\\beta}^T X_i}} \\) is the predicted probability that \\( Y_i = 1 \\)<br>\n",
        "&emsp;• \\( (X_i, Y_i) \\) are the observed input/output pairs<br>\n",
        "&emsp;• \\( \\boldsymbol{\\beta} \\) is the vector of model parameters (coefficients)\n",
        "</p>"
      ],
      "metadata": {
        "id": "o_Fx6uXb9TZf"
      },
      "id": "o_Fx6uXb9TZf"
    },
    {
      "cell_type": "markdown",
      "id": "b5e2aba8",
      "metadata": {
        "id": "b5e2aba8"
      },
      "source": [
        "Q5: What is the optimization problem that we try to solve when fitting logistic regression?  (8 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are trying to maximimize the log-likelihood. We are trying to find the best parameters to make our predictions as close to the labels as possible."
      ],
      "metadata": {
        "id": "kkqrKzFoFQ9m"
      },
      "id": "kkqrKzFoFQ9m"
    },
    {
      "cell_type": "markdown",
      "id": "c6990b0d",
      "metadata": {
        "id": "c6990b0d"
      },
      "source": [
        "Q6: What procedures can be used to solve the optimization problem underlying logistic regression? (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent; Stochastic Gradient descent, Coordinate descent"
      ],
      "metadata": {
        "id": "hlXljaFGE1Cy"
      },
      "id": "hlXljaFGE1Cy"
    },
    {
      "cell_type": "markdown",
      "id": "81d73b5d",
      "metadata": {
        "id": "81d73b5d"
      },
      "source": [
        "Q7: Derive the gradient of the log likelihood with respect to the parameters of the logistic regression model step by step. (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The log-likelihood function for logistic regression is:\n",
        "\n",
        "$$\n",
        "\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ Y_i \\log(p_i) + (1 - Y_i) \\log(1 - p_i) \\right]\n",
        "$$\n",
        "\n",
        "where the predicted probability is given by the sigmoid function:\n",
        "\n",
        "$$\n",
        "p_i = \\frac{1}{1 + e^{-\\beta^T X_i}}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "To find the gradient of the log-likelihood with respect to the parameters $( \\beta )$, we take the derivative:\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = \\sum_{i=1}^n (Y_i - p_i) X_i\n",
        "$$\n",
        "\n",
        "In vectorized form, this can be written as:\n",
        "\n",
        "$$\n",
        "\\nabla \\ell(\\boldsymbol{\\beta}) = X^T (Y - \\hat{p})\n",
        "$$\n",
        "\n",
        "where \\( X \\) is the feature matrix, \\( Y \\) is the vector of labels, and \\( \\hat{p} \\) is the vector of predicted probabilities.\n"
      ],
      "metadata": {
        "id": "m99AIeLcG9QU"
      },
      "id": "m99AIeLcG9QU"
    },
    {
      "cell_type": "markdown",
      "id": "87ff04c8",
      "metadata": {
        "id": "87ff04c8"
      },
      "source": [
        "### Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a2a527d-d1f5-411b-aae4-c0d55333239f",
      "metadata": {
        "id": "0a2a527d-d1f5-411b-aae4-c0d55333239f"
      },
      "source": [
        "**Implement Logistic Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "008739de",
      "metadata": {
        "id": "008739de"
      },
      "source": [
        "Q1: Write the function `generate_X(n,p)`, which returns randomly generated $X_1,\\cdots, X_n$, where $X_i \\in \\mathbb{R}^p$. You can sample the variables using a uniform distribution or a standard normal distribution. (8 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def generate_X(n, p):\n",
        "  X = np.random.randn(n, p) # sampled w/ standard normal distribution\n",
        "  return X\n"
      ],
      "metadata": {
        "id": "23jBAID3Jeqd"
      },
      "id": "23jBAID3Jeqd",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cb4598cf",
      "metadata": {
        "id": "cb4598cf"
      },
      "source": [
        "Q2: Write the function `generate_Y(X, beta, intercept)`, which generates outcomes for observations $X_1,\\cdots, X_p$ per a logistic regression model with coefficients $\\beta \\in \\mathbb{R}^{p}$ and intercept $\\beta_0$. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_Y(X, beta, intercept):\n",
        "  Y = 1 / (1 + np.exp(-(intercept + np.dot(X, beta))))\n",
        "  return Y"
      ],
      "metadata": {
        "id": "8QsceuZjO7Zv"
      },
      "id": "8QsceuZjO7Zv",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8e66ccc4",
      "metadata": {
        "id": "8e66ccc4"
      },
      "source": [
        "Q3: Generate some data using your functions above with $p=2$, $n=1000$, coefficients $\\beta=(0.5,2)$, and intercept $\\beta_0 = 1$. (7 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_Y(generate_X(1000, 2), np.array([0.5, 2]), 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "eTxVtkbhUKzS",
        "outputId": "eafe211e-287f-4c7d-b027-f58076bca3ed"
      },
      "id": "eTxVtkbhUKzS",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.83719082, 0.94620102, 0.86887447, 0.82987894, 0.29094019,\n",
              "       0.97300889, 0.95789745, 0.77027309, 0.96838763, 0.60644493,\n",
              "       0.98608963, 0.89286314, 0.67256986, 0.57783357, 0.92077026,\n",
              "       0.26776786, 0.82930834, 0.88556246, 0.4831727 , 0.88242961,\n",
              "       0.45003806, 0.28158977, 0.90077761, 0.92838579, 0.29630598,\n",
              "       0.37881193, 0.35443593, 0.71529253, 0.44206729, 0.60683721,\n",
              "       0.90930443, 0.84973206, 0.79916437, 0.89950969, 0.65355196,\n",
              "       0.98515519, 0.99359345, 0.54726719, 0.67260297, 0.87412156,\n",
              "       0.37304023, 0.59542444, 0.98125698, 0.67140525, 0.88332366,\n",
              "       0.50635475, 0.98682129, 0.87983278, 0.84635452, 0.89623524,\n",
              "       0.92070388, 0.92076271, 0.78513433, 0.48158891, 0.71596562,\n",
              "       0.50628061, 0.22245992, 0.96102783, 0.84967766, 0.52332464,\n",
              "       0.77911054, 0.68558464, 0.19621452, 0.8169554 , 0.95770279,\n",
              "       0.35439494, 0.9382545 , 0.96949107, 0.56245262, 0.56028134,\n",
              "       0.90322485, 0.9862899 , 0.86409956, 0.53737049, 0.81539539,\n",
              "       0.3367432 , 0.05531632, 0.82310677, 0.87232878, 0.99878337,\n",
              "       0.93856152, 0.79412707, 0.84038139, 0.94553927, 0.8760683 ,\n",
              "       0.03608301, 0.594173  , 0.39413705, 0.95341854, 0.94297938,\n",
              "       0.3362277 , 0.75667449, 0.95027002, 0.70328841, 0.99126915,\n",
              "       0.81309286, 0.95640167, 0.86894977, 0.9656517 , 0.32281419,\n",
              "       0.92531277, 0.72773739, 0.40004654, 0.83864449, 0.8444627 ,\n",
              "       0.73753672, 0.32193523, 0.9299588 , 0.74319602, 0.33489506,\n",
              "       0.38332944, 0.29206733, 0.89320592, 0.70553402, 0.96801884,\n",
              "       0.08521667, 0.17569948, 0.39595295, 0.35301101, 0.57417508,\n",
              "       0.07734687, 0.86581399, 0.33461571, 0.93719615, 0.9179497 ,\n",
              "       0.38437986, 0.96220679, 0.02178539, 0.56820856, 0.63002399,\n",
              "       0.10904869, 0.77380127, 0.98261133, 0.08177806, 0.14552696,\n",
              "       0.01807819, 0.06993107, 0.80968959, 0.75509745, 0.91860626,\n",
              "       0.27847967, 0.58429411, 0.66651546, 0.47183549, 0.09157405,\n",
              "       0.29621894, 0.12324146, 0.31856455, 0.61564892, 0.48566975,\n",
              "       0.71945546, 0.61119366, 0.93444283, 0.96928333, 0.85083733,\n",
              "       0.44314289, 0.71814767, 0.82123464, 0.00442718, 0.02272905,\n",
              "       0.07895522, 0.12595331, 0.77359742, 0.96792832, 0.25253241,\n",
              "       0.87899218, 0.32219045, 0.88555542, 0.46460286, 0.69308082,\n",
              "       0.91260398, 0.14513874, 0.22356903, 0.08862677, 0.80879353,\n",
              "       0.92216165, 0.89941676, 0.95225296, 0.9264243 , 0.9864872 ,\n",
              "       0.70422399, 0.94106047, 0.8144884 , 0.92217077, 0.8620188 ,\n",
              "       0.84713373, 0.72124759, 0.94294591, 0.03900978, 0.79505576,\n",
              "       0.81247212, 0.50748454, 0.93631687, 0.95911984, 0.63535406,\n",
              "       0.36562516, 0.08390664, 0.72029777, 0.95057154, 0.70621981,\n",
              "       0.65124354, 0.38226478, 0.40607641, 0.77263951, 0.62750945,\n",
              "       0.77416965, 0.95268857, 0.16752664, 0.02030852, 0.67502954,\n",
              "       0.13063572, 0.58388922, 0.91142013, 0.99843751, 0.57805122,\n",
              "       0.02725356, 0.42849217, 0.73199695, 0.96161719, 0.89625399,\n",
              "       0.98265189, 0.48494145, 0.90254355, 0.59128631, 0.99779272,\n",
              "       0.51353481, 0.74326926, 0.89940377, 0.43074234, 0.78228557,\n",
              "       0.73248268, 0.96121976, 0.99824785, 0.22091642, 0.90159787,\n",
              "       0.07948617, 0.90660778, 0.03800853, 0.93291877, 0.98318065,\n",
              "       0.67972236, 0.98515912, 0.89537654, 0.99531342, 0.17754661,\n",
              "       0.87564965, 0.81982328, 0.68848142, 0.85631088, 0.95088917,\n",
              "       0.56881289, 0.26918142, 0.87856516, 0.05345456, 0.90218559,\n",
              "       0.98923726, 0.78331165, 0.73076438, 0.18677824, 0.2242521 ,\n",
              "       0.94819531, 0.98439508, 0.40936831, 0.96286228, 0.99053104,\n",
              "       0.26186287, 0.97412498, 0.12710432, 0.06669058, 0.98836719,\n",
              "       0.98902988, 0.974813  , 0.0615924 , 0.83965569, 0.23180473,\n",
              "       0.85852078, 0.78244202, 0.9025166 , 0.93239665, 0.29916793,\n",
              "       0.03252214, 0.16299235, 0.55967119, 0.98362694, 0.05805701,\n",
              "       0.98222297, 0.07129914, 0.98351127, 0.5251263 , 0.43241416,\n",
              "       0.98599762, 0.86660676, 0.26363057, 0.84597537, 0.95040551,\n",
              "       0.32720692, 0.73615223, 0.16700691, 0.90424021, 0.66631121,\n",
              "       0.13035729, 0.90972755, 0.95074182, 0.74653268, 0.60540948,\n",
              "       0.93983477, 0.95954432, 0.53624343, 0.72587778, 0.87194585,\n",
              "       0.12096221, 0.38329334, 0.96455843, 0.90044089, 0.66484652,\n",
              "       0.75789385, 0.6851726 , 0.96243311, 0.03266196, 0.0658169 ,\n",
              "       0.45006145, 0.55386844, 0.01561707, 0.48273153, 0.63927446,\n",
              "       0.809273  , 0.17745696, 0.99851924, 0.26898125, 0.98776243,\n",
              "       0.6897731 , 0.81379376, 0.37232423, 0.06183582, 0.89789734,\n",
              "       0.68675582, 0.90878851, 0.60974499, 0.84991708, 0.14455349,\n",
              "       0.8596181 , 0.95965517, 0.8224979 , 0.50016086, 0.71072366,\n",
              "       0.2300134 , 0.96022973, 0.88740206, 0.85250028, 0.96636196,\n",
              "       0.98811747, 0.79651186, 0.02437312, 0.79535617, 0.9969529 ,\n",
              "       0.95363518, 0.99775785, 0.95973555, 0.95985533, 0.60776872,\n",
              "       0.86623056, 0.83319863, 0.91045376, 0.99115373, 0.81555429,\n",
              "       0.92610402, 0.688702  , 0.99644301, 0.22918124, 0.58484563,\n",
              "       0.83548144, 0.42145864, 0.53103399, 0.37867326, 0.58217482,\n",
              "       0.63435034, 0.95016127, 0.29620381, 0.55231895, 0.26418242,\n",
              "       0.56144012, 0.93575339, 0.22569915, 0.98665885, 0.91825773,\n",
              "       0.03360199, 0.50954007, 0.94332889, 0.87675381, 0.99592401,\n",
              "       0.77070428, 0.40266748, 0.98853878, 0.8977667 , 0.87824598,\n",
              "       0.6458896 , 0.34009354, 0.98781419, 0.43586027, 0.08329391,\n",
              "       0.8186273 , 0.17944515, 0.73267785, 0.99428269, 0.49262452,\n",
              "       0.70326351, 0.23393064, 0.49350863, 0.70597756, 0.46892002,\n",
              "       0.32730255, 0.58618103, 0.36055916, 0.85475946, 0.27244171,\n",
              "       0.72449842, 0.99534816, 0.94113108, 0.8042425 , 0.93404108,\n",
              "       0.093809  , 0.72767619, 0.14303654, 0.86201606, 0.3270189 ,\n",
              "       0.59193499, 0.99867983, 0.85988087, 0.87661693, 0.77212416,\n",
              "       0.73324394, 0.02549775, 0.04263476, 0.9438095 , 0.93992141,\n",
              "       0.66207086, 0.73655629, 0.43190764, 0.95517291, 0.62747794,\n",
              "       0.8961995 , 0.828797  , 0.45556338, 0.78370295, 0.69352304,\n",
              "       0.90792311, 0.96115371, 0.92474091, 0.65948827, 0.97298562,\n",
              "       0.98068262, 0.63186149, 0.65064461, 0.5373944 , 0.81784125,\n",
              "       0.90288111, 0.67528467, 0.61000026, 0.85402585, 0.92185295,\n",
              "       0.78004412, 0.0017891 , 0.29757384, 0.95406596, 0.02959461,\n",
              "       0.12980705, 0.92565986, 0.24292011, 0.68709652, 0.37685863,\n",
              "       0.97144422, 0.80494182, 0.64573521, 0.90597083, 0.97685782,\n",
              "       0.2632593 , 0.93951432, 0.03847892, 0.39294441, 0.86576908,\n",
              "       0.86258445, 0.69949367, 0.91644241, 0.4131554 , 0.98219508,\n",
              "       0.16007782, 0.36693406, 0.75840388, 0.85112702, 0.70517478,\n",
              "       0.80930238, 0.44270946, 0.7185585 , 0.4119782 , 0.02173271,\n",
              "       0.73558188, 0.10647098, 0.87306418, 0.17126044, 0.98897219,\n",
              "       0.77348164, 0.14158859, 0.21422699, 0.79023939, 0.72162726,\n",
              "       0.88180738, 0.9704142 , 0.97358687, 0.02898601, 0.96973743,\n",
              "       0.36271535, 0.31074937, 0.16661765, 0.58186804, 0.68611509,\n",
              "       0.51304165, 0.80658228, 0.57964775, 0.97151835, 0.02035265,\n",
              "       0.45306385, 0.26957319, 0.51859375, 0.39307818, 0.97294585,\n",
              "       0.56060974, 0.68603581, 0.90620102, 0.89292763, 0.63109272,\n",
              "       0.76920624, 0.97218738, 0.46081002, 0.1550518 , 0.75568971,\n",
              "       0.79383943, 0.64928713, 0.41823298, 0.12324619, 0.92936346,\n",
              "       0.27589745, 0.28905096, 0.06035852, 0.50674744, 0.90844851,\n",
              "       0.92109574, 0.97869736, 0.93603946, 0.81242147, 0.6014619 ,\n",
              "       0.74080006, 0.53946709, 0.57650918, 0.90875416, 0.91748017,\n",
              "       0.56127956, 0.88250985, 0.94164641, 0.82160544, 0.72484017,\n",
              "       0.10513108, 0.76326759, 0.90543332, 0.75903117, 0.06729352,\n",
              "       0.95259625, 0.43106928, 0.86948775, 0.86495893, 0.88654814,\n",
              "       0.49790257, 0.97904263, 0.43070586, 0.15852187, 0.79643662,\n",
              "       0.28387737, 0.49824457, 0.15735733, 0.44151421, 0.91141295,\n",
              "       0.78101   , 0.2778828 , 0.12688364, 0.87991953, 0.26647345,\n",
              "       0.5751437 , 0.325534  , 0.62391169, 0.81334196, 0.95030374,\n",
              "       0.53534517, 0.14182741, 0.03499808, 0.65831514, 0.73452251,\n",
              "       0.89944872, 0.7188783 , 0.90884869, 0.71038403, 0.98925534,\n",
              "       0.9416834 , 0.23069369, 0.82018806, 0.68858112, 0.16575676,\n",
              "       0.37110671, 0.64674451, 0.73259714, 0.93547521, 0.47534977,\n",
              "       0.89985481, 0.92696075, 0.90689233, 0.09040197, 0.5934516 ,\n",
              "       0.44508327, 0.66754519, 0.98658611, 0.97064644, 0.37732823,\n",
              "       0.84370757, 0.09531407, 0.11986853, 0.97614724, 0.88579445,\n",
              "       0.65752247, 0.8411708 , 0.45011939, 0.71852201, 0.79765817,\n",
              "       0.88713544, 0.98316195, 0.8976441 , 0.74503614, 0.83387426,\n",
              "       0.95174443, 0.26243254, 0.86787758, 0.68148357, 0.66218457,\n",
              "       0.67177117, 0.91449731, 0.32534351, 0.97285149, 0.96149908,\n",
              "       0.78275298, 0.87223707, 0.95373175, 0.22790886, 0.06746909,\n",
              "       0.97368171, 0.45853052, 0.7284838 , 0.42078737, 0.96917994,\n",
              "       0.12511645, 0.89664253, 0.07611453, 0.36053706, 0.84444463,\n",
              "       0.46876556, 0.95779295, 0.4580463 , 0.85559452, 0.59585204,\n",
              "       0.2513199 , 0.10809044, 0.26189456, 0.98099221, 0.95519203,\n",
              "       0.14378415, 0.99800486, 0.87569153, 0.93040737, 0.02138143,\n",
              "       0.34749694, 0.4301281 , 0.82887355, 0.87910998, 0.91895816,\n",
              "       0.63207832, 0.2090237 , 0.94715745, 0.92497655, 0.02725973,\n",
              "       0.71399042, 0.98021205, 0.69849592, 0.42631859, 0.44869228,\n",
              "       0.685448  , 0.99981597, 0.77246848, 0.95202372, 0.18577891,\n",
              "       0.63566214, 0.84643267, 0.47533047, 0.21100347, 0.66072848,\n",
              "       0.91786834, 0.08764569, 0.66507577, 0.97860029, 0.98342075,\n",
              "       0.71268109, 0.98459171, 0.51074952, 0.13298916, 0.52756371,\n",
              "       0.85726714, 0.08590963, 0.53157622, 0.92264968, 0.24055834,\n",
              "       0.38072038, 0.02495292, 0.03640571, 0.57216404, 0.97878951,\n",
              "       0.11610576, 0.99851872, 0.96900271, 0.95813082, 0.93957623,\n",
              "       0.79420278, 0.27561089, 0.98708773, 0.97611304, 0.71416497,\n",
              "       0.38646126, 0.9935874 , 0.82932903, 0.58205798, 0.84272857,\n",
              "       0.92382445, 0.9358135 , 0.43222767, 0.64021845, 0.5862953 ,\n",
              "       0.60806827, 0.19193778, 0.90603981, 0.56744957, 0.79214062,\n",
              "       0.26784013, 0.78613335, 0.97115021, 0.92800539, 0.15226283,\n",
              "       0.99229849, 0.90903988, 0.00485304, 0.94083412, 0.91434158,\n",
              "       0.85139926, 0.15214426, 0.89127294, 0.2229491 , 0.97419766,\n",
              "       0.64795885, 0.73625844, 0.84467377, 0.42531231, 0.99689751,\n",
              "       0.66529337, 0.93243785, 0.6437928 , 0.98840081, 0.16900545,\n",
              "       0.67429291, 0.70040455, 0.5388027 , 0.21349585, 0.59486817,\n",
              "       0.32880993, 0.65812062, 0.27324006, 0.73993993, 0.99236133,\n",
              "       0.57693653, 0.89574325, 0.79195285, 0.8222863 , 0.08222515,\n",
              "       0.69413319, 0.97093542, 0.56675754, 0.89309196, 0.0275364 ,\n",
              "       0.21461705, 0.94241931, 0.6643291 , 0.98551649, 0.76798618,\n",
              "       0.33122876, 0.28660367, 0.67277365, 0.17162358, 0.95876875,\n",
              "       0.41221039, 0.10986715, 0.07760027, 0.36476498, 0.93960244,\n",
              "       0.91587159, 0.98204692, 0.98193822, 0.9960023 , 0.49418115,\n",
              "       0.61379006, 0.36025005, 0.92008402, 0.99159147, 0.99414379,\n",
              "       0.40844734, 0.27663952, 0.82910468, 0.72047275, 0.78393149,\n",
              "       0.93201354, 0.92766023, 0.97562817, 0.50663629, 0.7586379 ,\n",
              "       0.95755366, 0.13378586, 0.18347293, 0.68105442, 0.96378429,\n",
              "       0.95247171, 0.97896848, 0.86109151, 0.86296697, 0.56771944,\n",
              "       0.15539172, 0.61394268, 0.35083157, 0.94690308, 0.81255801,\n",
              "       0.80381024, 0.02952207, 0.91166769, 0.87556569, 0.87503787,\n",
              "       0.8364442 , 0.99819143, 0.20097989, 0.98301955, 0.94844324,\n",
              "       0.96970122, 0.33163312, 0.97804806, 0.06014445, 0.7009284 ,\n",
              "       0.64898417, 0.24162757, 0.42690584, 0.57199002, 0.9756005 ,\n",
              "       0.80230742, 0.88329938, 0.11490905, 0.96445132, 0.99956107,\n",
              "       0.19865923, 0.92207599, 0.533847  , 0.97886301, 0.98002444,\n",
              "       0.19795329, 0.46548715, 0.96218432, 0.97850136, 0.552835  ,\n",
              "       0.10525584, 0.98430332, 0.76410656, 0.90466457, 0.1251554 ,\n",
              "       0.99557112, 0.16309546, 0.99387364, 0.03322255, 0.91544557,\n",
              "       0.70579226, 0.92526668, 0.5456173 , 0.7869028 , 0.23110855,\n",
              "       0.78074102, 0.95496324, 0.64627901, 0.36766349, 0.9975222 ,\n",
              "       0.90706296, 0.83229032, 0.99061787, 0.71991308, 0.83140155,\n",
              "       0.99888752, 0.64439912, 0.87209122, 0.84677484, 0.06371046,\n",
              "       0.92656333, 0.23193844, 0.79819418, 0.58895728, 0.29222504,\n",
              "       0.07372078, 0.99017019, 0.51631208, 0.75833619, 0.9661123 ,\n",
              "       0.59082434, 0.91295144, 0.91505421, 0.73012945, 0.81869736,\n",
              "       0.90765349, 0.50681634, 0.03816827, 0.37810973, 0.82488125,\n",
              "       0.9036593 , 0.85813553, 0.8908254 , 0.11629461, 0.97241903,\n",
              "       0.98100706, 0.8479459 , 0.63367875, 0.43545288, 0.89849541,\n",
              "       0.32203108, 0.93978238, 0.75570494, 0.35228515, 0.22240861,\n",
              "       0.08366045, 0.92771527, 0.85922155, 0.2711054 , 0.9534034 ,\n",
              "       0.98321251, 0.86951701, 0.12239456, 0.82308913, 0.86982971,\n",
              "       0.51915052, 0.99746649, 0.15654592, 0.66444023, 0.77815374,\n",
              "       0.87906664, 0.79168712, 0.98927247, 0.10995062, 0.75870086,\n",
              "       0.72695031, 0.04328556, 0.83280447, 0.85662338, 0.17001183,\n",
              "       0.11847051, 0.29228155, 0.59045268, 0.8121911 , 0.29372749,\n",
              "       0.91325073, 0.75477083, 0.0907053 , 0.48203689, 0.32221487,\n",
              "       0.80765519, 0.79562796, 0.83679524, 0.97999237, 0.30322233,\n",
              "       0.45235053, 0.99548948, 0.82172168, 0.75590681, 0.90958985,\n",
              "       0.54334031, 0.97522377, 0.51497015, 0.12974398, 0.71415055,\n",
              "       0.9940884 , 0.569437  , 0.66772236, 0.0205256 , 0.09977091,\n",
              "       0.78688451, 0.45144135, 0.99965561, 0.51642152, 0.87431064])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c5a40b8",
      "metadata": {
        "id": "0c5a40b8"
      },
      "source": [
        "Q4: Implement a function that runs gradient descent `run_gradient_descent(X, Y, alpha, num_iterations, initial_betas)`. Make sure to vectorize your code. (Otherwise it will run really slowly.) (15 points)\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_gradient_descent(X, Y, alpha, num_iterations, initial_betas):\n",
        "  \"\"\"\n",
        "\n",
        "  Creating a function that will take:\n",
        "    X: input data (like hours studied, age, etc.)\n",
        "    Y: the correct answers (0s and 1s)\n",
        "    alpha: Learning rate — how big of a step you take each time\n",
        "    num_iterations: How many steps you take\n",
        "    initial_betas: Your starting guesses for the weights\n",
        "    Runs gradient descent to optimize the parameters of a logistic regression model.\n",
        "\n",
        "  Args:\n",
        "    X: A NumPy array of shape (n, p) representing the feature matrix.\n",
        "    Y: A NumPy array of shape (n,) representing the target variable.\n",
        "    alpha: The learning rate.\n",
        "    num_iterations: The number of iterations to run gradient descent.\n",
        "    initial_betas: The initial values for the model parameters.\n",
        "\n",
        "  Returns:\n",
        "    A NumPy array representing the optimized model parameters.\n",
        "  \"\"\"\n",
        "\n",
        "  n, p = X.shape\n",
        "  betas = initial_betas.copy()\n",
        "\n",
        "  for _ in range(num_iterations):\n",
        "    # Calculate the predicted probabilities.\n",
        "    z = np.dot(X, betas)\n",
        "    predicted_probs = 1 / (1 + np.exp(-z))\n",
        "\n",
        "    # Calculate the gradient of the log-likelihood.\n",
        "    gradient = np.dot(X.T, (predicted_probs - Y)) / n\n",
        "\n",
        "    # Update the parameters using the gradient descent update rule.\n",
        "    betas -= alpha * gradient\n",
        "\n",
        "  return betas"
      ],
      "metadata": {
        "id": "2fILNhIuV_s0"
      },
      "id": "2fILNhIuV_s0",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "62965710",
      "metadata": {
        "id": "62965710"
      },
      "source": [
        "\n",
        "Q5: Apply your implementation of gradient descent to the generated data to estimate the parameters. How close are they to the true parameters? (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate data with p=2, n=1000, beta=(0.5, 2), and intercept=1\n",
        "n = 1000\n",
        "p = 2\n",
        "beta_true = np.array([0.5, 2])\n",
        "intercept_true = 1\n",
        "X = generate_X(n, p)\n",
        "Y = generate_Y(X, beta_true, intercept_true)\n",
        "# Round the probabilities to get binary outcomes for demonstration\n",
        "Y_binary = np.round(Y)\n",
        "\n",
        "\n",
        "# Apply gradient descent\n",
        "alpha = 0.1\n",
        "num_iterations = 1000\n",
        "initial_betas = np.zeros(p)\n",
        "estimated_betas = run_gradient_descent(X, Y_binary, alpha, num_iterations, initial_betas)\n",
        "\n",
        "print(\"True Betas:\", beta_true)\n",
        "print(\"Estimated Betas:\", estimated_betas)\n",
        "print(\"True Intercept:\", intercept_true) # Note: our gradient descent doesn't estimate the intercept\n",
        "\n",
        "# Calculate the difference between true and estimated parameters.\n",
        "diff = np.abs(beta_true - estimated_betas)\n",
        "print(\"\\nDifference between true and estimated betas:\", diff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aPMFUNLgWAPV",
        "outputId": "140b161a-1e04-47d4-ffc5-7b01b1c3db10"
      },
      "id": "aPMFUNLgWAPV",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Betas: [0.5 2. ]\n",
            "Estimated Betas: [0.71526653 2.96143018]\n",
            "True Intercept: 1\n",
            "\n",
            "Difference between true and estimated betas: [0.21526653 0.96143018]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ede89a8",
      "metadata": {
        "id": "1ede89a8"
      },
      "source": [
        "\n",
        "Q6: Rerun your implementation of gradient descent but with a different initialization. Are the estimated parameters the same as that in Q5? (8 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rerun gradient descent with a different initialization\n",
        "initial_betas_new = np.array([1.0, 0.0])  # Different initial values, now as floats\n",
        "estimated_betas_new = run_gradient_descent(X, Y_binary, alpha, num_iterations, initial_betas_new)\n",
        "\n",
        "print(\"\\nEstimated Betas (new initialization):\", estimated_betas_new)\n",
        "\n",
        "# Compare the new estimates with the previous ones\n",
        "diff_new = np.abs(estimated_betas - estimated_betas_new)\n",
        "print(\"\\nDifference between estimated betas (different initializations):\", diff_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YMsMyTYMWAwK",
        "outputId": "65e16452-8aff-4448-867f-cd2adf4a188d"
      },
      "id": "YMsMyTYMWAwK",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Estimated Betas (new initialization): [0.71590693 2.96419368]\n",
            "\n",
            "Difference between estimated betas (different initializations): [0.0006404 0.0027635]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e8e692c",
      "metadata": {
        "id": "2e8e692c"
      },
      "source": [
        "**Comparing your solution against scikit-learn**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c0a4309",
      "metadata": {
        "id": "6c0a4309"
      },
      "source": [
        "Q7: Apply `sklearn.linear_model.LogisticRegression¶` to your generated data to estimate the parameters of a logistic regression model. ( 7 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize and fit the Logistic Regression model\n",
        "logreg = LogisticRegression(fit_intercept=True, solver='lbfgs') # fit_intercept=True to estimate the intercept\n",
        "logreg.fit(X, Y_binary)\n",
        "\n",
        "# Print the estimated coefficients and intercept\n",
        "print(\"\\nEstimated coefficients (scikit-learn):\", logreg.coef_)\n",
        "print(\"Estimated intercept (scikit-learn):\", logreg.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TujpwtcLWB3C",
        "outputId": "94f22c46-a177-4600-93c4-465900ce0857"
      },
      "id": "TujpwtcLWB3C",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Estimated coefficients (scikit-learn): [[2.13496064 7.95817214]]\n",
            "Estimated intercept (scikit-learn): [4.28658054]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2491c54",
      "metadata": {
        "id": "d2491c54"
      },
      "source": [
        "Q8: Are the answers the same as that from your implementation? (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No. They are not. Sci-kit learn's coefficients are much higher than mine."
      ],
      "metadata": {
        "id": "3MMwVNCPJbHs"
      },
      "id": "3MMwVNCPJbHs"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FeE5VuN5WCL4"
      },
      "id": "FeE5VuN5WCL4"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}